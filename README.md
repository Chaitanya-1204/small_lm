# small_lm


## Dataset 

* There are two train Train datasets one contains 10M words and other contain 100M words.


--- 

## Building Tokenizers 

* Using BPE algorithms to train tokenizers From scratch. 