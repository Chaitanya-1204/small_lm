{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ed965",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50790147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from clean import (\n",
    "    cleanup_bnc_spoken,\n",
    "    cleanup_childes,\n",
    "    cleanup_gutenberg,\n",
    "    cleanup_open_subtitles,\n",
    "    cleanup_simple_wiki,\n",
    "    cleanup_switchboard\n",
    ")\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer , decoders , models , pre_tokenizers , trainers , processors\n",
    ")\n",
    "\n",
    "from tokenizers.normalizers import NFKC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd075f",
   "metadata": {},
   "source": [
    "# Checking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a5b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['bnc_spoken', 'childes' , 'gutenberg', 'open_subtitles', 'simple_wiki' , 'switchboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38b2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064345c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnc_spoken: 932,497 words\n",
      "childes: 2,839,591 words\n",
      "gutenberg: 2,539,489 words\n",
      "open_subtitles: 2,041,868 words\n",
      "simple_wiki: 1,453,539 words\n",
      "switchboard: 146,789 words\n",
      "Total: 9.95M words\n"
     ]
    }
   ],
   "source": [
    "words = 0 \n",
    "for cat in categories:\n",
    "    data_path = os.path.join('datasets/train_10M', cat +\".train\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words += len(text.split())\n",
    "    print(f'{cat}: {len(text.split()):,} words')\n",
    "print(f'Total: {words/1e6:.2f}M words') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4512bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnc_spoken: 7,760,721 words\n",
      "childes: 28,903,287 words\n",
      "gutenberg: 26,371,234 words\n",
      "open_subtitles: 19,963,117 words\n",
      "simple_wiki: 14,674,311 words\n",
      "switchboard: 1,342,029 words\n",
      "Total: 99.01M words\n"
     ]
    }
   ],
   "source": [
    "words = 0 \n",
    "for cat in categories:\n",
    "    data_path = os.path.join('datasets/train_100M', cat + \".train\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words += len(text.split())\n",
    "    print(f'{cat}: {len(text.split()):,} words')\n",
    "print(f'Total: {words/1e6:.2f}M words') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9512d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnc_spoken: 1,252,593 words\n",
      "childes: 2,716,591 words\n",
      "gutenberg: 2,819,070 words\n",
      "open_subtitles: 2,077,019 words\n",
      "simple_wiki: 1,405,366 words\n",
      "switchboard: 148,340 words\n",
      "Total: 10.42M words\n"
     ]
    }
   ],
   "source": [
    "words = 0 \n",
    "for cat in categories:\n",
    "    data_path = os.path.join('datasets/dev', cat + \".dev\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words += len(text.split())\n",
    "    print(f'{cat}: {len(text.split()):,} words')\n",
    "print(f'Total: {words/1e6:.2f}M words') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14281fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnc_spoken: 932,334 words\n",
      "childes: 2,700,128 words\n",
      "gutenberg: 2,404,516 words\n",
      "open_subtitles: 1,949,898 words\n",
      "simple_wiki: 1,300,077 words\n",
      "switchboard: 167,133 words\n",
      "Total: 9.45M words\n"
     ]
    }
   ],
   "source": [
    "words = 0 \n",
    "for cat in categories:\n",
    "    data_path = os.path.join('datasets/test', cat + \".test\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words += len(text.split())\n",
    "    print(f'{cat}: {len(text.split()):,} words')\n",
    "print(f'Total: {words/1e6:.2f}M words') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8588a6",
   "metadata": {},
   "source": [
    "# cleaned dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcfb68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "bnc_spoken train_10M: 932497 -> 927102 words\n",
      "childes train_10M: 2839591 -> 2839397 words\n",
      "gutenberg train_10M: 2539489 -> 2539489 words\n",
      "open_subtitles train_10M: 2041868 -> 2041523 words\n",
      "simple_wiki train_10M: 1453539 -> 1441982 words\n",
      "switchboard train_10M: 146789 -> 146789 words\n",
      "--------------------------------------------------------------------------------\n",
      "bnc_spoken train_100M: 7760721 -> 7713524 words\n",
      "childes train_100M: 28903287 -> 28901073 words\n",
      "gutenberg train_100M: 26371234 -> 26371234 words\n",
      "open_subtitles train_100M: 19963117 -> 19960443 words\n",
      "simple_wiki train_100M: 14674311 -> 14564235 words\n",
      "switchboard train_100M: 1342029 -> 1342029 words\n",
      "--------------------------------------------------------------------------------\n",
      "bnc_spoken dev: 1252593 -> 1245432 words\n",
      "childes dev: 2716591 -> 2716381 words\n",
      "gutenberg dev: 2819070 -> 2819070 words\n",
      "open_subtitles dev: 2077019 -> 2076847 words\n",
      "simple_wiki dev: 1405366 -> 1395373 words\n",
      "switchboard dev: 148340 -> 148340 words\n",
      "--------------------------------------------------------------------------------\n",
      "bnc_spoken test: 932334 -> 927134 words\n",
      "childes test: 2700128 -> 2700013 words\n",
      "gutenberg test: 2404516 -> 2404516 words\n",
      "open_subtitles test: 1949898 -> 1949738 words\n",
      "simple_wiki test: 1300077 -> 1290159 words\n",
      "switchboard test: 167133 -> 167133 words\n"
     ]
    }
   ],
   "source": [
    "DATA_SPLIT = ['train_10M', 'train_100M', 'dev', 'test']\n",
    "\n",
    "categories_and_function = {\n",
    "    'bnc_spoken': cleanup_bnc_spoken,\n",
    "    'childes': cleanup_childes,\n",
    "    'gutenberg': cleanup_gutenberg,\n",
    "    'open_subtitles': cleanup_open_subtitles,\n",
    "    'simple_wiki': cleanup_simple_wiki,\n",
    "    'switchboard': cleanup_switchboard\n",
    "}\n",
    "\n",
    "seq_length = 128\n",
    "\n",
    "\n",
    "\n",
    "for split in DATA_SPLIT:\n",
    "    print(\"--\" * 40)\n",
    "    if split == 'train_10M' or split == 'train_100M':\n",
    "        for cat , func in categories_and_function.items():\n",
    "            cleaned_data_dir = os.path.join(\"cleaned_datasets\" , split)\n",
    "            cleaned_data_path = os.path.join(cleaned_data_dir , cat  + \".train\")\n",
    "            os.makedirs(cleaned_data_dir , exist_ok = True)\n",
    "            data_path = os.path.join(\"datasets\" , split , cat + \".train\")\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            normal_words = len(text.split())\n",
    "            cleaned_text = func(text , seq_length)\n",
    "            cleaned_words = len(cleaned_text.split())\n",
    "            print(f'{cat} {split}: {normal_words} -> {cleaned_words} words')\n",
    "            with open(cleaned_data_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "    elif split == 'dev':\n",
    "        for cat , func in categories_and_function.items():\n",
    "            cleaned_data_dir = os.path.join(\"cleaned_datasets\" , split )\n",
    "            cleaned_data_path = os.path.join(cleaned_data_dir , cat  + \".dev\")\n",
    "            os.makedirs(cleaned_data_dir , exist_ok = True)\n",
    "            data_path = os.path.join('datasets', split, cat + \".dev\")\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            normal_words = len(text.split())\n",
    "            cleaned_text = func(text , seq_length)\n",
    "            cleaned_words = len(cleaned_text.split())\n",
    "            print(f'{cat} {split}: {normal_words} -> {cleaned_words} words')\n",
    "            with open(cleaned_data_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "    elif split == 'test':\n",
    "        for cat , func in categories_and_function.items():\n",
    "            cleaned_data_dir = os.path.join(\"cleaned_datasets\" , split )\n",
    "            cleaned_data_path = os.path.join(cleaned_data_dir , cat  + \".test\")\n",
    "            os.makedirs(cleaned_data_dir , exist_ok = True)\n",
    "            data_path = os.path.join('datasets', split, cat + \".test\")\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            normal_words = len(text.split())\n",
    "            cleaned_text = func(text , seq_length)\n",
    "            cleaned_words = len(cleaned_text.split())\n",
    "            print(f'{cat} {split}: {normal_words} -> {cleaned_words} words')\n",
    "            with open(cleaned_data_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1bc13",
   "metadata": {},
   "source": [
    "# Building Tokenizers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3391fea",
   "metadata": {},
   "source": [
    "## train_10M tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d397cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for cat in categories:\n",
    "    paths.append(os.path.join('cleaned_datasets/train_10M', cat + \".train\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e947c423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned_datasets/train_10M/bnc_spoken.train',\n",
       " 'cleaned_datasets/train_10M/childes.train',\n",
       " 'cleaned_datasets/train_10M/gutenberg.train',\n",
       " 'cleaned_datasets/train_10M/open_subtitles.train',\n",
       " 'cleaned_datasets/train_10M/simple_wiki.train',\n",
       " 'cleaned_datasets/train_10M/switchboard.train']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d658c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4aafa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.train(paths, trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4309d7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer/tokenizer_train10M.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('tokenizer', exist_ok=True)\n",
    "tokenizer_path = os.path.join('tokenizer', 'tokenizer_train10M.json')\n",
    "tokenizer.save(tokenizer_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1239bd6",
   "metadata": {},
   "source": [
    "## train_100M tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d89da63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for cat in categories:\n",
    "    paths.append(os.path.join('cleaned_datasets/train_100M', cat + \".train\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c451a2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned_datasets/train_100M/bnc_spoken.train',\n",
       " 'cleaned_datasets/train_100M/childes.train',\n",
       " 'cleaned_datasets/train_100M/gutenberg.train',\n",
       " 'cleaned_datasets/train_100M/open_subtitles.train',\n",
       " 'cleaned_datasets/train_100M/simple_wiki.train',\n",
       " 'cleaned_datasets/train_100M/switchboard.train']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35c324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7e1f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.train(paths, trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b087d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer/tokenizer_train100M.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_path = os.path.join('tokenizer', 'tokenizer_train100M.json')\n",
    "tokenizer.save(tokenizer_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b540e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from tokenizer/tokenizer_train100M.json\n",
      "Encoded text: ['ĠHello', 'Ġworld', '!', 'ĠThis', 'Ġis', 'Ġa', 'Ġtest', '.']\n",
      "Encoded IDs: [2556, 1229, 3, 757, 260, 192, 2775, 16]\n",
      "Decoded text:  Hello world! This is a test.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "\n",
    "text = \"Hello world! This is a test.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"Encoded text: {encoded.tokens}\")\n",
    "print(f\"Encoded IDs: {encoded.ids}\")\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d3831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
